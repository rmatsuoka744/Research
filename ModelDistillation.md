# 工事中

# **[Model Distillation](https://arxiv.org/pdf/1503.02531.pdf)**
高精度で大規模なモデルを教師モデルとして、パラメータ数を削減した蒸留モデルを訓練する手法。最近のLLMでは大規模なデータセットを莫大なパラメータを持つモデルで長い時間をかけてトレーニングすることが多い。しかし、その場合では計算量がどんどん増えていき、結果として一度の推論を行うたびにハイスペックなマシンが必要になっている。そのため、元のモデルと同じような確率分布を出力することが可能なモデルを訓練する手法であるModel Distillationが注目されている。

# **Model**
1. **[DistilBERT](https://arxiv.org/pdf/1910.01108.pdf)**
    - Hugging Faceが提供する、元のBERTモデルを蒸留して小さなサイズにしたモデル。BERTの性能を維持しつつ、軽量かつ高速に推論ができるようになっている。

2. **[TinyBERT](https://arxiv.org/pdf/1909.10351.pdf)**
    - DistilBERTよりも更に小さなモデルを目指している蒸留モデル。蒸留手法によりBERTから知識を抽出し、小さなBERTを生成する。性能とリソース効率のトレードオフが考慮されている。

3. **[MobileBERT](https://arxiv.org/pdf/2004.02984.pdf)**
    - Googleが提供するモバイルデバイス上での実用性を考慮して設計されたモデル。BERTの性能を維持しつつ、モバイル環境での推論に適している。