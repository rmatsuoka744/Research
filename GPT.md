# 工事中

# **GPT**
GPTとは、Generative pre-trained transformerのことであり、事前学習を行った生成モデルである。自然言語処理タスクにおいて驚異的な性能を発揮するTransformerをベースとしている。2018年にGPT-1が発表され、現在ではGPT-4まで発表されている。
GPTは、単純に言えば「大規模なデータから事前学習した確率分布に従って文章に続く単語を推論することで、それらしい文章を生成する」モデルといえる。

# **[GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)**
2018年にOpenAIによってリリースされたGPTシリーズの初めてのモデル。

1. **アーキテクチャ**
![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Full_GPT_architecture.png/800px-Full_GPT_architecture.png)
    - GPT-1はTransformerアーキテクチャを採用したモデルである。Attentionメカニズムによって、時系列データに対して高精度の推論が可能である。
    - GPT-1は自己回帰性を持つモデルであり、文章を生成する際、推論された次のトークンは元の文章に結合され、次のトークンを推論する際に利用される。
    - Transformerのエンコーダ・デコーダを使用せず、デコーダのみを使用している。
    - エンコーダ・デコーダの両方を使用する場合、デコーダには開始トークンのみが与えられ、次のトークンを推論する際に逐次エンコーダ側からクロスアテンションを行う。
    - デコーダのみを使用する場合は、質問文や翻訳前の文章に開始トークンを与え、それがそのままデコーダに供給され、セルフアテンションによって推論を行う。

2. **モデルサイズ**
    - 117Bパラメータ

3. **トレーニングデータ**
    - 4.5GBの書籍データ

# **[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)**
2019年に発表されたモデルで、GPT-1と比較しておよそ12.8倍のパラメータを持つ。

1. **アーキテクチャ**
    - 基本的なアーキテクチャはGPT-1と同様だが、Transformerの層の数など、ハイパーパラメータに違いがある。

2. **モデルサイズ**
    - 1.5B~175Bパラメータ

3. **トレーニングデータ**
    - 40GBのWebテキストデータ

# **[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)**
2020年に発表されたモデルであり、トランスフォーマーモデルの規模を大幅に拡張し、多様なタスクへ適用可能とした。

1. **アーキテクチャ**
    - 同様に、基本的なアーキテクチャは同じだが、ハイパーパラメータが変更されている。

2. **モデルサイズ**
    - 175B~1750Bパラメータ

3. **トレーニングデータ**
    - 570GBの大規模かつ多様なデータセット（Webテキスト、Wikipedia、書籍など）

# **GPT-3.5**
2022年に発表されたモデルであり、GPT-3からパラメータが増加している。GPT-3と比較して劇的な差はないが、現在のChatGPTの無料版で使用されている。

# **[GPT-4](https://arxiv.org/pdf/2303.08774.pdf)**
2023年に発表されたモデル。安全性の観点から、設計は非公開。GPT-3.5から精度が大幅に向上している。マルチモーダルなタスクにも対応しており、画像とテキストなどを入力できるようになっている。ChatGPT Plusで、月額20$から利用できる。